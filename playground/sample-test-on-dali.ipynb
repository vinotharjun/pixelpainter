{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install DALI from https://www.github.com/NVIDIA/DALI to run this example.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5f048835a953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mnvidia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdali\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDALIClassificationIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mnvidia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdali\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nvidia'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5f048835a953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mnvidia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdali\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please install DALI from https://www.github.com/NVIDIA/DALI to run this example.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Please install DALI from https://www.github.com/NVIDIA/DALI to run this example."
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "\n",
    "import threading\n",
    "from torch.multiprocessing import Event\n",
    "from torch._six import queue\n",
    "try:\n",
    "    from nvidia.dali.plugin.pytorch import DALIClassificationIterator\n",
    "    from nvidia.dali.pipeline import Pipeline\n",
    "    import nvidia.dali.ops as ops\n",
    "    import nvidia.dali.types as types\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install DALI from https://www.github.com/NVIDIA/DALI to run this example.\")\n",
    "\n",
    "\n",
    "class HybridTrainPipe(Pipeline):\n",
    "    \"\"\"\n",
    "    DALI Train Pipeline\n",
    "    Based on the official example: https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py\n",
    "    In comparison to the example, the CPU backend does more computation on CPU, reducing GPU load & memory use.\n",
    "    This dataloader implements ImageNet style training preprocessing, namely:\n",
    "    -random resized crop\n",
    "    -random horizontal flip\n",
    "    batch_size (int): how many samples per batch to load\n",
    "    num_threads (int): how many DALI workers to use for data loading.\n",
    "    device_id (int): GPU device ID\n",
    "    data_dir (str): Directory to dataset.  Format should be the same as torchvision dataloader,\n",
    "    containing train & val subdirectories, with image class subfolders\n",
    "    crop (int): Image output size (typically 224 for ImageNet)\n",
    "    mean (tuple): Image mean value for each channel\n",
    "    std (tuple): Image standard deviation value for each channel\n",
    "    local_rank (int, optional, default = 0) â€“ Id of the part to read\n",
    "    world_size (int, optional, default = 1) - Partition the data into this many parts (used for multiGPU training)\n",
    "    dali_cpu (bool, optional, default = False) - Use DALI CPU mode instead of GPU\n",
    "    shuffle (bool, optional, default = True) - Shuffle the dataset each epoch\n",
    "    fp16 (bool, optional, default = False) - Output the data in fp16 instead of fp32 (GPU mode only)\n",
    "    min_crop_size (float, optional, default = 0.08) - Minimum random crop size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, num_threads, device_id, data_dir, crop,\n",
    "                 mean, std, local_rank=0, world_size=1, dali_cpu=False, shuffle=True, fp16=False,\n",
    "                 min_crop_size=0.08):\n",
    "\n",
    "        # As we're recreating the Pipeline at every epoch, the seed must be -1 (random seed)\n",
    "        super(HybridTrainPipe, self).__init__(batch_size, num_threads, device_id, seed=-1)\n",
    "\n",
    "        # Enabling read_ahead slowed down processing ~40%\n",
    "        self.input = ops.FileReader(file_root=data_dir, shard_id=local_rank, num_shards=world_size,\n",
    "                                    random_shuffle=shuffle)\n",
    "\n",
    "        # Let user decide which pipeline works best with the chosen model\n",
    "        if dali_cpu:\n",
    "            decode_device = \"cpu\"\n",
    "            self.dali_device = \"cpu\"\n",
    "            self.flip = ops.Flip(device=self.dali_device)\n",
    "        else:\n",
    "            decode_device = \"mixed\"\n",
    "            self.dali_device = \"gpu\"\n",
    "\n",
    "            output_dtype = types.FLOAT\n",
    "            if self.dali_device == \"gpu\" and fp16:\n",
    "                output_dtype = types.FLOAT16\n",
    "\n",
    "            self.cmn = ops.CropMirrorNormalize(device=\"gpu\",\n",
    "                                               output_dtype=output_dtype,\n",
    "                                               output_layout=types.NCHW,\n",
    "                                               crop=(crop, crop),\n",
    "                                               image_type=types.RGB,\n",
    "                                               mean=mean,\n",
    "                                               std=std,)\n",
    "\n",
    "        # To be able to handle all images from full-sized ImageNet, this padding sets the size of the internal\n",
    "        # nvJPEG buffers without additional reallocations\n",
    "        device_memory_padding = 211025920 if decode_device == 'mixed' else 0\n",
    "        host_memory_padding = 140544512 if decode_device == 'mixed' else 0\n",
    "        self.decode = ops.ImageDecoderRandomCrop(device=decode_device, output_type=types.RGB,\n",
    "                                                 device_memory_padding=device_memory_padding,\n",
    "                                                 host_memory_padding=host_memory_padding,\n",
    "                                                 random_aspect_ratio=[0.8, 1.25],\n",
    "                                                 random_area=[min_crop_size, 1.0],\n",
    "                                                 num_attempts=100)\n",
    "\n",
    "        # Resize as desired.  To match torchvision data loader, use triangular interpolation.\n",
    "        self.res = ops.Resize(device=self.dali_device, resize_x=crop, resize_y=crop,\n",
    "                              interp_type=types.INTERP_TRIANGULAR)\n",
    "\n",
    "        self.coin = ops.CoinFlip(probability=0.5)\n",
    "        print('DALI \"{0}\" variant'.format(self.dali_device))\n",
    "\n",
    "    def define_graph(self):\n",
    "        rng = self.coin()\n",
    "        self.jpegs, self.labels = self.input(name=\"Reader\")\n",
    "\n",
    "        # Combined decode & random crop\n",
    "        images = self.decode(self.jpegs)\n",
    "\n",
    "        # Resize as desired\n",
    "        images = self.res(images)\n",
    "\n",
    "        if self.dali_device == \"gpu\":\n",
    "            output = self.cmn(images, mirror=rng)\n",
    "        else:\n",
    "            # CPU backend uses torch to apply mean & std\n",
    "            output = self.flip(images, horizontal=rng)\n",
    "\n",
    "        self.labels = self.labels.gpu()\n",
    "        return [output, self.labels]\n",
    "\n",
    "\n",
    "class HybridValPipe(Pipeline):\n",
    "    \"\"\"\n",
    "    DALI Validation Pipeline\n",
    "    Based on the official example: https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py\n",
    "    In comparison to the example, the CPU backend does more computation on CPU, reducing GPU load & memory use.\n",
    "    This dataloader implements ImageNet style validation preprocessing, namely:\n",
    "    -resize to specified size\n",
    "    -center crop to desired size\n",
    "    batch_size (int): how many samples per batch to load\n",
    "    num_threads (int): how many DALI workers to use for data loading.\n",
    "    device_id (int): GPU device ID\n",
    "    data_dir (str): Directory to dataset.  Format should be the same as torchvision dataloader,\n",
    "        containing train & val subdirectories, with image class subfolders\n",
    "    crop (int): Image output size (typically 224 for ImageNet)\n",
    "    size (int): Resize size (typically 256 for ImageNet)\n",
    "    mean (tuple): Image mean value for each channel\n",
    "    std (tuple): Image standard deviation value for each channel\n",
    "    local_rank (int, optional, default = 0) â€“ Id of the part to read\n",
    "    world_size (int, optional, default = 1) - Partition the data into this many parts (used for multiGPU training)\n",
    "    dali_cpu (bool, optional, default = False) - Use DALI CPU mode instead of GPU\n",
    "    shuffle (bool, optional, default = True) - Shuffle the dataset each epoch\n",
    "    fp16 (bool, optional, default = False) - Output the data in fp16 instead of fp32 (GPU mode only)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, num_threads, device_id, data_dir, crop, size,\n",
    "                 mean, std, local_rank=0, world_size=1, dali_cpu=False, shuffle=False, fp16=False):\n",
    "\n",
    "        # As we're recreating the Pipeline at every epoch, the seed must be -1 (random seed)\n",
    "        super(HybridValPipe, self).__init__(batch_size, num_threads, device_id, seed=-1)\n",
    "\n",
    "        # Enabling read_ahead slowed down processing ~40%\n",
    "        # Note: initial_fill is for the shuffle buffer.  As we only want to see every example once, this is set to 1\n",
    "        self.input = ops.FileReader(file_root=data_dir, shard_id=local_rank, num_shards=world_size, random_shuffle=shuffle, initial_fill=1)\n",
    "        if dali_cpu:\n",
    "            decode_device = \"cpu\"\n",
    "            self.dali_device = \"cpu\"\n",
    "            self.crop = ops.Crop(device=\"cpu\", crop=(crop, crop))\n",
    "\n",
    "        else:\n",
    "            decode_device = \"mixed\"\n",
    "            self.dali_device = \"gpu\"\n",
    "\n",
    "            output_dtype = types.FLOAT\n",
    "            if fp16:\n",
    "                output_dtype = types.FLOAT16\n",
    "\n",
    "            self.cmnp = ops.CropMirrorNormalize(device=\"gpu\",\n",
    "                                                output_dtype=output_dtype,\n",
    "                                                output_layout=types.NCHW,\n",
    "                                                crop=(crop, crop),\n",
    "                                                image_type=types.RGB,\n",
    "                                                mean=mean,\n",
    "                                                std=std)\n",
    "\n",
    "        self.decode = ops.ImageDecoder(device=decode_device, output_type=types.RGB)\n",
    "\n",
    "        # Resize to desired size.  To match torchvision dataloader, use triangular interpolation\n",
    "        self.res = ops.Resize(device=self.dali_device, resize_shorter=size, interp_type=types.INTERP_TRIANGULAR)\n",
    "\n",
    "    def define_graph(self):\n",
    "        self.jpegs, self.labels = self.input(name=\"Reader\")\n",
    "        images = self.decode(self.jpegs)\n",
    "        images = self.res(images)\n",
    "        if self.dali_device == 'gpu':\n",
    "            output = self.cmnp(images)\n",
    "        else:\n",
    "            # CPU backend uses torch to apply mean & std\n",
    "            output = self.crop(images)\n",
    "\n",
    "        self.labels = self.labels.gpu()\n",
    "        return [output, self.labels]\n",
    "\n",
    "\n",
    "class DaliIterator():\n",
    "    \"\"\"\n",
    "    Wrapper class to decode the DALI iterator output & provide iterator that functions the same as torchvision\n",
    "    pipelines (Pipeline): DALI pipelines\n",
    "    size (int): Number of examples in set\n",
    "    Note: allow extra inputs to keep compatibility with CPU iterator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pipelines, size, **kwargs):\n",
    "        self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, size=size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.ceil(self._dali_iterator._size / self._dali_iterator.batch_size))\n",
    "\n",
    "\n",
    "class DaliIteratorGPU(DaliIterator):\n",
    "    \"\"\"\n",
    "    Wrapper class to decode the DALI iterator output & provide iterator that functions the same as torchvision\n",
    "    pipelines (Pipeline): DALI pipelines\n",
    "    size (int): Number of examples in set\n",
    "    Note: allow extra inputs to keep compatibility with CPU iterator\n",
    "    \"\"\"\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            data = next(self._dali_iterator)\n",
    "        except StopIteration:\n",
    "            print('Resetting DALI loader')\n",
    "            self._dali_iterator.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        # Decode the data output\n",
    "        input = data[0]['data']\n",
    "        target = data[0]['label'].squeeze().long()\n",
    "\n",
    "        return input, target\n",
    "\n",
    "\n",
    "def _preproc_worker(dali_iterator, cuda_stream, fp16, mean, std, output_queue, proc_next_input, done_event, pin_memory):\n",
    "    \"\"\"\n",
    "    Worker function to parse DALI output & apply final pre-processing steps\n",
    "    \"\"\"\n",
    "\n",
    "    while not done_event.is_set():\n",
    "        # Wait until main thread signals to proc_next_input -- normally once it has taken the last processed input\n",
    "        proc_next_input.wait()\n",
    "        proc_next_input.clear()\n",
    "\n",
    "        if done_event.is_set():\n",
    "            print('Shutting down preproc thread')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = next(dali_iterator)\n",
    "\n",
    "            # Decode the data output\n",
    "            input_orig = data[0]['data']\n",
    "            target = data[0]['label'].squeeze().long()  # DALI should already output target on device\n",
    "\n",
    "            # Copy to GPU and apply final processing in separate CUDA stream\n",
    "            with torch.cuda.stream(cuda_stream):\n",
    "                input = input_orig\n",
    "                if pin_memory:\n",
    "                    input = input.pin_memory()\n",
    "                    del input_orig  # Save memory\n",
    "                input = input.cuda(non_blocking=True)\n",
    "\n",
    "                input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "                # Input tensor is kept as 8-bit integer for transfer to GPU, to save bandwidth\n",
    "                if fp16:\n",
    "                    input = input.half()\n",
    "                else:\n",
    "                    input = input.float()\n",
    "\n",
    "                input = input.sub_(mean).div_(std)\n",
    "\n",
    "            # Put the result on the queue\n",
    "            output_queue.put((input, target))\n",
    "\n",
    "        except StopIteration:\n",
    "            print('Resetting DALI loader')\n",
    "            dali_iterator.reset()\n",
    "            output_queue.put(None)\n",
    "\n",
    "\n",
    "class DaliIteratorCPU(DaliIterator):\n",
    "    \"\"\"\n",
    "    Wrapper class to decode the DALI iterator output & provide iterator that functions the same as torchvision\n",
    "    Note that permutation to channels first, converting from 8 bit to float & normalization are all performed on GPU\n",
    "    pipelines (Pipeline): DALI pipelines\n",
    "    size (int): Number of examples in set\n",
    "    fp16 (bool): Use fp16 as output format, f32 otherwise\n",
    "    mean (tuple): Image mean value for each channel\n",
    "    std (tuple): Image standard deviation value for each channel\n",
    "    pin_memory (bool): Transfer input tensor to pinned memory, before moving to GPU\n",
    "    \"\"\"\n",
    "    def __init__(self, fp16=False, mean=(0., 0., 0.), std=(1., 1., 1.), pin_memory=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print('Using DALI CPU iterator')\n",
    "        self.stream = torch.cuda.Stream()\n",
    "\n",
    "        self.fp16 = fp16\n",
    "        self.mean = torch.tensor(mean).cuda().view(1, 3, 1, 1)\n",
    "        self.std = torch.tensor(std).cuda().view(1, 3, 1, 1)\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        if self.fp16:\n",
    "            self.mean = self.mean.half()\n",
    "            self.std = self.std.half()\n",
    "\n",
    "        self.proc_next_input = Event()\n",
    "        self.done_event = Event()\n",
    "        self.output_queue = queue.Queue(maxsize=5)\n",
    "        self.preproc_thread = threading.Thread(\n",
    "            target=_preproc_worker,\n",
    "            kwargs={'dali_iterator': self._dali_iterator, 'cuda_stream': self.stream, 'fp16': self.fp16, 'mean': self.mean, 'std': self.std, 'proc_next_input': self.proc_next_input, 'done_event': self.done_event, 'output_queue': self.output_queue, 'pin_memory': self.pin_memory})\n",
    "        self.preproc_thread.daemon = True\n",
    "        self.preproc_thread.start()\n",
    "\n",
    "        self.proc_next_input.set()\n",
    "\n",
    "    def __next__(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        data = self.output_queue.get()\n",
    "        self.proc_next_input.set()\n",
    "        if data is None:\n",
    "            raise StopIteration\n",
    "        return data\n",
    "\n",
    "    def __del__(self):\n",
    "        self.done_event.set()\n",
    "        self.proc_next_input.set()\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        self.preproc_thread.join()\n",
    "\n",
    "\n",
    "class DaliIteratorCPUNoPrefetch(DaliIterator):\n",
    "    \"\"\"\n",
    "    Wrapper class to decode the DALI iterator output & provide iterator that functions the same as torchvision\n",
    "    Note that permutation to channels first, converting from 8 bit to float & normalization are all performed on GPU\n",
    "    pipelines (Pipeline): DALI pipelines\n",
    "    size (int): Number of examples in set\n",
    "    fp16 (bool): Use fp16 as output format, f32 otherwise\n",
    "    mean (tuple): Image mean value for each channel\n",
    "    std (tuple): Image standard deviation value for each channel\n",
    "    pin_memory (bool): Transfer input tensor to pinned memory, before moving to GPU\n",
    "    \"\"\"\n",
    "    def __init__(self, fp16, mean, std, pin_memory=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print('Using DALI CPU iterator')\n",
    "\n",
    "        self.stream = torch.cuda.Stream()\n",
    "\n",
    "        self.fp16 = fp16\n",
    "        self.mean = torch.tensor(mean).cuda().view(1, 3, 1, 1)\n",
    "        self.std = torch.tensor(std).cuda().view(1, 3, 1, 1)\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        if self.fp16:\n",
    "            self.mean = self.mean.half()\n",
    "            self.std = self.std.half()\n",
    "\n",
    "    def __next__(self):\n",
    "        data = next(self._dali_iterator)\n",
    "\n",
    "        # Decode the data output\n",
    "        input = data[0]['data']\n",
    "        target = data[0]['label'].squeeze().long()  # DALI should already output target on device\n",
    "\n",
    "        # Copy to GPU & apply final processing in seperate CUDA stream\n",
    "        input = input.cuda(non_blocking=True)\n",
    "\n",
    "        input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Input tensor is transferred to GPU as 8 bit, to save bandwidth\n",
    "        if self.fp16:\n",
    "            input = input.half()\n",
    "        else:\n",
    "            input = input.float()\n",
    "\n",
    "        input = input.sub_(self.mean).div_(self.std)\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import importlib\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "def clear_memory(verbose=False):\n",
    "    stt = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()  # https://forums.fast.ai/t/clearing-gpu-memory-pytorch/14637\n",
    "    gc.collect()\n",
    "\n",
    "    if verbose:\n",
    "        print('Cleared memory.  Time taken was %f secs' % (time.time() - stt))\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"\n",
    "    Pytorch Dataloader, with torchvision or Nvidia DALI CPU/GPU pipelines.\n",
    "    This dataloader implements ImageNet style training preprocessing, namely:\n",
    "    -random resized crop\n",
    "    -random horizontal flip\n",
    "    And ImageNet style validation preprocessing, namely:\n",
    "    -resize to specified size\n",
    "    -center crop to desired size\n",
    "    data_dir (str): Directory to dataset.  Format should be the same as torchvision dataloader,\n",
    "    batch_size (int): how many samples per batch to load\n",
    "    size (int): Output size (typically 224 for ImageNet)\n",
    "    val_size (int): Validation pipeline resize size (typically 256 for ImageNet)\n",
    "    workers (int): how many workers to use for data loading\n",
    "    world_size (int, optional, default = 1) - Partition the data into this many parts (used for multiGPU training)\n",
    "    cuda (bool): Output tensors on CUDA, CPU otherwise\n",
    "    use_dali (bool): Use Nvidia DALI backend, torchvision otherwise\n",
    "    dali_cpu (bool): Use Nvidia DALI cpu backend, GPU backend otherwise\n",
    "    fp16 (bool, optional, default = False) - Output the data in fp16 instead of fp32\n",
    "    mean (tuple): Image mean value for each channel\n",
    "    std (tuple): Image standard deviation value for each channel\n",
    "    pin_memory (bool): Transfer CPU tensor to pinned memory before transfer to GPU (torchvision only)\n",
    "    pin_memory_dali (bool): Transfer CPU tensor to pinned memory before transfer to GPU (dali only)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 batch_size,\n",
    "                 size=224,\n",
    "                 val_batch_size=None,\n",
    "                 val_size=256,\n",
    "                 min_crop_size=0.08,\n",
    "                 workers=4,\n",
    "                 world_size=1,\n",
    "                 cuda=True,\n",
    "                 use_dali=False,\n",
    "                 dali_cpu=True,\n",
    "                 fp16=False,\n",
    "                 mean=(0.485 * 255, 0.456 * 255, 0.406 * 255),\n",
    "                 std=(0.229 * 255, 0.224 * 255, 0.225 * 255),\n",
    "                 pin_memory=True,\n",
    "                 pin_memory_dali=False,\n",
    "                 ):\n",
    "\n",
    "            self.batch_size = batch_size\n",
    "            self.size = size\n",
    "            self.val_batch_size = val_batch_size\n",
    "            self.min_crop_size = min_crop_size\n",
    "            self.workers = workers\n",
    "            self.world_size = world_size\n",
    "            self.cuda = cuda\n",
    "            self.use_dali = use_dali\n",
    "            self.dali_cpu = dali_cpu\n",
    "            self.fp16 = fp16\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            self.pin_memory = pin_memory\n",
    "            self.pin_memory_dali = pin_memory_dali\n",
    "\n",
    "            self.val_size = val_size\n",
    "            if self.val_size is None:\n",
    "                self.val_size = self.size\n",
    "\n",
    "            if self.val_batch_size is None:\n",
    "                self.val_batch_size = self.batch_size\n",
    "\n",
    "            # Data loading code\n",
    "            self.traindir = \"../../storage/inpainting-dataset\"\n",
    "#             self.valdir = os.path.join(data_dir, 'val')\n",
    "\n",
    "            # DALI Dataloader\n",
    "            if self.use_dali:\n",
    "                print('Using Nvidia DALI dataloader')\n",
    "#                 assert len(datasets.ImageFolder(self.valdir)) % self.val_batch_size == 0, 'Validation batch size must divide validation dataset size cleanly...  DALI has problems otherwise.'\n",
    "                self._build_dali_pipeline()\n",
    "\n",
    "            # Standard torchvision dataloader\n",
    "            else:\n",
    "                print('Using torchvision dataloader')\n",
    "                self._build_torchvision_pipeline()\n",
    "\n",
    "\n",
    "    def _build_torchvision_pipeline(self):\n",
    "        preproc_train = [transforms.RandomResizedCrop(self.size, scale=(self.min_crop_size, 1.0)),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         ]\n",
    "\n",
    "        preproc_val = [transforms.Resize(self.val_size),\n",
    "                       transforms.CenterCrop(self.size),\n",
    "                       ]\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(self.traindir, transforms.Compose(preproc_train))\n",
    "        val_dataset = datasets.ImageFolder(self.valdir, transforms.Compose(preproc_val))\n",
    "\n",
    "        self.train_sampler = None\n",
    "        self.val_sampler = None\n",
    "\n",
    "        if self.world_size > 1:\n",
    "            raise NotImplementedError('distributed support not tested yet...')\n",
    "            self.train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "            self.val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=(self.train_sampler is None),\n",
    "            num_workers=self.workers, pin_memory=self.pin_memory, sampler=self.train_sampler, collate_fn=fast_collate)\n",
    "\n",
    "        self.val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=self.val_batch_size, shuffle=False, num_workers=self.workers,\n",
    "            pin_memory=self.pin_memory, sampler=self.val_sampler, collate_fn=fast_collate)\n",
    "\n",
    "    def _build_dali_pipeline(self, val_on_cpu=True):\n",
    "        assert self.world_size == 1, 'Distributed support not tested yet'\n",
    "\n",
    "        iterator_train = DaliIteratorGPU\n",
    "        if self.dali_cpu:\n",
    "            iterator_train = DaliIteratorCPU\n",
    "\n",
    "        self.train_pipe = HybridTrainPipe(batch_size=self.batch_size, num_threads=self.workers, device_id=0,\n",
    "                                          data_dir=self.traindir, crop=self.size, dali_cpu=self.dali_cpu,\n",
    "                                          mean=self.mean, std=self.std, local_rank=0,\n",
    "                                          world_size=self.world_size, shuffle=True, fp16=self.fp16, min_crop_size=self.min_crop_size)\n",
    "\n",
    "        self.train_pipe.build()\n",
    "        self.train_loader = iterator_train(pipelines=self.train_pipe, size=self.get_nb_train() / self.world_size, fp16=self.fp16, mean=self.mean, std=self.std, pin_memory=self.pin_memory_dali)\n",
    "\n",
    "#         iterator_val = DaliIteratorGPU\n",
    "#         if val_on_cpu:\n",
    "#             iterator_val = DaliIteratorCPU\n",
    "\n",
    "#         self.val_pipe = HybridValPipe(batch_size=self.val_batch_size, num_threads=self.workers, device_id=0,\n",
    "#                                       data_dir=self.valdir, crop=self.size, size=self.val_size, dali_cpu=val_on_cpu,\n",
    "#                                       mean=self.mean, std=self.std, local_rank=0,\n",
    "#                                       world_size=self.world_size, shuffle=False, fp16=self.fp16)\n",
    "\n",
    "#         self.val_pipe.build()\n",
    "#         self.val_loader = iterator_val(pipelines=self.val_pipe, size=self.get_nb_val() / self.world_size, fp16=self.fp16, mean=self.mean, std=self.std, pin_memory=self.pin_memory_dali)\n",
    "\n",
    "    def _get_torchvision_loader(self, loader):\n",
    "        return TorchvisionIterator(loader=loader,\n",
    "                                   cuda=self.cuda,\n",
    "                                   fp16=self.fp16,\n",
    "                                   mean=self.mean,\n",
    "                                   std=self.std,\n",
    "                                   )\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        \"\"\"\n",
    "        Creates & returns an iterator for the training dataset\n",
    "        :return: Dataset iterator object\n",
    "        \"\"\"\n",
    "        if self.use_dali:\n",
    "            return self.train_loader\n",
    "        return self._get_torchvision_loader(loader=self.train_loader)\n",
    "\n",
    "    def get_val_loader(self):\n",
    "        \"\"\"\n",
    "        Creates & returns an iterator for the training dataset\n",
    "        :return: Dataset iterator object\n",
    "        \"\"\"\n",
    "        if self.use_dali:\n",
    "            return self.val_loader\n",
    "        return self._get_torchvision_loader(loader=self.val_loader)\n",
    "\n",
    "    def get_nb_train(self):\n",
    "        \"\"\"\n",
    "        :return: Number of training examples\n",
    "        \"\"\"\n",
    "        if self.use_dali:\n",
    "            return int(self.train_pipe.epoch_size(\"Reader\"))\n",
    "        return len(datasets.ImageFolder(self.traindir))\n",
    "\n",
    "    def get_nb_val(self):\n",
    "        \"\"\"\n",
    "        :return: Number of validation examples\n",
    "        \"\"\"\n",
    "        if self.use_dali:\n",
    "            return int(self.val_pipe.epoch_size(\"Reader\"))\n",
    "        return len(datasets.ImageFolder(self.valdir))\n",
    "\n",
    "    def prep_for_val(self):\n",
    "        self.reset(val_on_cpu=False)\n",
    "\n",
    "    # This is needed only for DALI\n",
    "    def reset(self, val_on_cpu=True):\n",
    "        if self.use_dali:\n",
    "            clear_memory()\n",
    "\n",
    "            # Currently we need to delete & rebuild the dali pipeline every epoch,\n",
    "            # due to a memory leak somewhere in DALI\n",
    "            print('Recreating DALI dataloaders to reduce memory usage')\n",
    "            del self.train_loader, self.val_loader, self.train_pipe, self.val_pipe\n",
    "            clear_memory()\n",
    "\n",
    "            # taken from: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "            importlib.reload(dali)\n",
    "            from dali import HybridTrainPipe, HybridValPipe, DaliIteratorCPU, DaliIteratorGPU\n",
    "\n",
    "            self._build_dali_pipeline(val_on_cpu=val_on_cpu)\n",
    "\n",
    "    def set_train_batch_size(self, train_batch_size):\n",
    "        self.batch_size = train_batch_size\n",
    "        if self.use_dali:\n",
    "            del self.train_loader, self.val_loader, self.train_pipe, self.val_pipe\n",
    "            self._build_dali_pipeline()\n",
    "        else:\n",
    "            del self.train_sampler, self.val_sampler, self.train_loader, self.val_loader\n",
    "            self._build_torchvision_pipeline()\n",
    "\n",
    "    def get_nb_classes(self):\n",
    "        \"\"\"\n",
    "        :return: The number of classes in the dataset - as indicated by the validation dataset\n",
    "        \"\"\"\n",
    "        return len(datasets.ImageFolder(self.valdir).classes)\n",
    "\n",
    "\n",
    "def fast_collate(batch):\n",
    "    \"\"\"Convert batch into tuple of X and Y tensors.\"\"\"\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    w = imgs[0].size[0]\n",
    "    h = imgs[0].size[1]\n",
    "    tensor = torch.zeros((len(imgs), 3, h, w), dtype=torch.uint8)\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.asarray(img, dtype=np.uint8)\n",
    "        if (nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "\n",
    "    return tensor, targets\n",
    "\n",
    "\n",
    "class TorchvisionIterator():\n",
    "    \"\"\"\n",
    "    Iterator to perform final data pre-processing steps:\n",
    "    -transfer to device (done on 8 bit tensor to reduce bandwidth requirements)\n",
    "    -convert to fp32/fp16 tensor\n",
    "    -apply mean/std scaling\n",
    "    loader (DataLoader): Torchvision Dataloader\n",
    "    cuda (bool): Transfer tensor to CUDA device\n",
    "    fp16 (bool): Convert tensor to fp16 instead of fp32\n",
    "    mean (tuple): Image mean value for each channel\n",
    "    std (tuple): Image standard deviation value for each channel\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 loader,\n",
    "                 cuda=False,\n",
    "                 fp16=False,\n",
    "                 mean=(0., 0., 0.),\n",
    "                 std=(1., 1., 1.),\n",
    "                 ):\n",
    "        print('Using Torchvision iterator')\n",
    "        self.loader = iter(loader)\n",
    "        self.cuda = cuda\n",
    "        self.mean = torch.tensor(mean).view(1, 3, 1, 1)\n",
    "        self.std = torch.tensor(std).view(1, 3, 1, 1)\n",
    "        self.fp16 = fp16\n",
    "\n",
    "        if self.cuda:\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "\n",
    "        if self.fp16:\n",
    "            self.mean = self.mean.half()\n",
    "            self.std = self.std.half()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        input, target = next(self.loader)\n",
    "\n",
    "        if self.cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        if self.fp16:\n",
    "            input = input.half()\n",
    "        else:\n",
    "            input = input.float()\n",
    "\n",
    "        input = input.sub_(self.mean).div_(self.std)\n",
    "\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nvidia DALI dataloader\n",
      "DALI \"cpu\" variant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.\n",
      "  _iterator_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DALI CPU iterator\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(data_dir=\"./\",\n",
    "                      batch_size=16,\n",
    "                      val_batch_size=16,\n",
    "                      workers=4,\n",
    "                      world_size=1,\n",
    "                      use_dali=True,\n",
    "                      dali_cpu=True,\n",
    "                      fp16=True,\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dataset.get_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i, data in enumerate(loader):\n",
    "    images = data[0].cuda(non_blocking=True)\n",
    "    labels = data[1].cuda(non_blocking=True)\n",
    "end = time.time()\n",
    "test_time = end-start\n",
    "print('[DALI] end test dataloader iteration')\n",
    "    # print('[DALI] iteration time: %fs [train],  %fs [test]' % (train_time, test_time))\n",
    "print('[DALI] iteration time: %fs [test]' % (test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
