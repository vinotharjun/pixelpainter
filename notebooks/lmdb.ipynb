{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import fire\n",
    "import glob\n",
    "import lmdb\n",
    "import logging\n",
    "import pyarrow\n",
    "import lz4framed\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import jpeg4py as jpeg\n",
    "from itertools import tee\n",
    "from typing import Generator, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format= '[%(asctime)s] [%(pathname)s:%(lineno)d] %(levelname)s - %(message)s',\n",
    "                    datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "DATA_DIRECTORY = '.././dataset/'\n",
    "IMAGE_NAMES_FILE = 'image_names.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmdb_connection = lmdb.open(\"./data.lmdb\", subdir=False,\n",
    "                                map_size=int(2e11), readonly=False,\n",
    "                                meminit=False, map_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def list_files_in_folder(folder_path: str) -> Generator:\n",
    "    return (folder_path+file_name__str for file_name__str in os.listdir(folder_path))\n",
    "\n",
    "\n",
    "def read_image_safely(image_file_name: str) -> np.array:\n",
    "    try:\n",
    "        return np.array(Image.open(image_file_name).convert(\"RGB\")).astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        return np.array([], dtype=np.uint8)\n",
    "\n",
    "\n",
    "def serialize_and_compress(obj: Any):\n",
    "    return lz4framed.compress(pyarrow.serialize(obj).to_buffer())\n",
    "\n",
    "\n",
    "def extract_image_name(image_path: str) -> str:\n",
    "    return image_path.split('/').pop(-1)\n",
    "\n",
    "\n",
    "def resize(image_array, size=(256, 256)):\n",
    "    if image_array.size == 0:\n",
    "        return image_array\n",
    "    return cv2.resize(image_array, dsize=size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def convert(image_folder: str, lmdb_output_path: str, write_freq: int=5000):\n",
    "    assert os.path.isdir(image_folder), f\"Image folder '{image_folder}' does not exist\"\n",
    "    assert not os.path.isfile(lmdb_output_path), f\"LMDB store '{lmdb_output_path} already exists\"\n",
    "    assert not os.path.isdir(lmdb_output_path), f\"LMDB store name should a file, found directory: {lmdb_output_path}\"\n",
    "    assert write_freq > 0, f\"Write frequency should be a positive number, found {write_freq}\"\n",
    "\n",
    "    logger.info(f\"Creating LMDB store: {lmdb_output_path}\")\n",
    "\n",
    "    image_file: Generator = list_files_in_folder(image_folder)\n",
    "    image_file, image_file__iter_c1, image_file__iter_c2, image_file__iter_c3 = tee(image_file, 4)\n",
    "\n",
    "\n",
    "    img_path_img_array__tuples = map(lambda tup: (tup[0], read_image_safely(tup[1])),\n",
    "                                     zip(image_file__iter_c1, image_file__iter_c2))\n",
    "\n",
    "    lmdb_connection = lmdb.open(lmdb_output_path, subdir=False,\n",
    "                                map_size=int(1e11), readonly=False,\n",
    "                                meminit=False, map_async=True)\n",
    "\n",
    "    lmdb_txn = lmdb_connection.begin(write=True)\n",
    "    total_records = 0\n",
    "\n",
    "    try:\n",
    "        for idx, (img_path, img_arr) in enumerate(tqdm(img_path_img_array__tuples)):\n",
    "            img_idx: bytes = u\"{}\".format(idx).encode('ascii')\n",
    "            img_name: str = extract_image_name(image_path=img_path)\n",
    "            img_name: bytes = u\"{}\".format(img_name).encode('ascii')\n",
    "            if idx < 5:\n",
    "                print(idx,img_name, img_arr.size, img_arr.shape)\n",
    "                logger.debug(img_idx, img_name, img_arr.size, img_arr.shape)\n",
    "            lmdb_txn.put(img_idx, serialize_and_compress((img_name, img_arr.tobytes(), img_arr.shape)))\n",
    "            total_records += 1\n",
    "            if idx % write_freq == 0:\n",
    "                lmdb_txn.commit()\n",
    "                lmdb_txn = lmdb_connection.begin(write=True)\n",
    "    except TypeError:\n",
    "        print(traceback.format_exc())\n",
    "        logger.error(traceback.format_exc())\n",
    "        lmdb_connection.close()\n",
    "        raise\n",
    "\n",
    "    lmdb_txn.commit()\n",
    "\n",
    "    logger.info(\"Finished writing image data. Total records: {}\".format(total_records))\n",
    "\n",
    "    logger.info(\"Writing store metadata\")\n",
    "    image_keys__list = [u'{}'.format(k).encode('ascii') for k in range(total_records)]\n",
    "    with lmdb_connection.begin(write=True) as lmdb_txn:\n",
    "        lmdb_txn.put(b'__keys__', serialize_and_compress(image_keys__list))\n",
    "\n",
    "    logger.info(\"Flushing data buffers to disk\")\n",
    "    lmdb_connection.sync()\n",
    "    lmdb_connection.close()\n",
    "#     print(image_file_iter_c3)\n",
    "\n",
    "    # -- store the order in which files were inserted into LMDB store -- #\n",
    "    pd.Series(image_file__iter_c3).apply(extract_image_name).to_csv(os.path.join(DATA_DIRECTORY, IMAGE_NAMES_FILE),\n",
    "                                                                    index=False, header=False)\n",
    "    logger.info(\"Finished creating LMDB store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\".././dataset/256/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:20:59] [<ipython-input-8-01c279a695e3>:32] INFO - Creating LMDB store: .././dataset/lmdb-store.db\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'000002b66c9c498e_resized.jpg' 196608 (256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b'000002b97e5471a0_resized.jpg' 196608 (256, 256, 3)\n",
      "2 b'000002c707c9895e_resized.jpg' 196608 (256, 256, 3)\n",
      "3 b'0000048549557964_resized.jpg' 196608 (256, 256, 3)\n",
      "4 b'000004f4400f6ec5_resized.jpg' 196608 (256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241978it [42:56, 93.93it/s] \n"
     ]
    },
    {
     "ename": "MapFullError",
     "evalue": "mdb_put: MDB_MAP_FULL: Environment mapsize limit reached",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMapFullError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-60235f8f13f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDATA_DIRECTORY\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"lmdb-store.db\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-01c279a695e3>\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(image_folder, lmdb_output_path, write_freq)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mlmdb_txn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialize_and_compress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mtotal_records\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwrite_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMapFullError\u001b[0m: mdb_put: MDB_MAP_FULL: Environment mapsize limit reached"
     ]
    }
   ],
   "source": [
    "convert(datadir,DATA_DIRECTORY+\"lmdb-store.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:57:24] [C:\\Users\\user\\Anaconda3\\envs\\myenv\\lib\\site-packages\\nonechucks\\__init__.py:18] WARNING - nonechucks may not work properly with this version of PyTorch (1.5.0). It has only been tested on PyTorch versions 1.0, 1.1, and 1.2\n"
     ]
    }
   ],
   "source": [
    "# lmdbloader.py\n",
    "\n",
    "import os\n",
    "import lmdb\n",
    "import pyarrow\n",
    "import lz4framed\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "import nonechucks as nc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class InvalidFileException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class LMDBDataset(Dataset):\n",
    "    def __init__(self, lmdb_store_path, transform=None):\n",
    "        super().__init__()\n",
    "        assert os.path.isfile(lmdb_store_path), f\"LMDB store '{lmdb_store_path} does not exist\"\n",
    "        assert not os.path.isdir(lmdb_store_path), f\"LMDB store name should a file, found directory: {lmdb_store_path}\"\n",
    "\n",
    "        self.lmdb_store_path = lmdb_store_path\n",
    "        self.lmdb_connection = lmdb.open(lmdb_store_path,\n",
    "                                         subdir=False, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "\n",
    "        with self.lmdb_connection.begin(write=False) as lmdb_txn:\n",
    "            self.length = lmdb_txn.stat()['entries'] - 1\n",
    "            self.keys = pyarrow.deserialize(lz4framed.decompress(lmdb_txn.get(b'__keys__')))\n",
    "            print(f\"Total records: {len(self.keys), self.length}\")\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lmdb_value = None\n",
    "        with self.lmdb_connection.begin(write=False) as txn:\n",
    "            lmdb_value = txn.get(self.keys[index])\n",
    "        assert lmdb_value is not None, f\"Read empty record for key: {self.keys[index]}\"\n",
    "\n",
    "        img_name, img_arr, img_shape = LMDBDataset.decompress_and_deserialize(lmdb_value=lmdb_value)\n",
    "        image = np.frombuffer(img_arr, dtype=np.uint8).reshape(img_shape)\n",
    "        if image.size == 0:\n",
    "            raise InvalidFileException(\"Invalid file found, skipping\")\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def decompress_and_deserialize(lmdb_value: Any):\n",
    "        return pyarrow.deserialize(lz4framed.decompress(lmdb_value))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LMDBDataset(DATA_DIRECTORY+\"lmdb-store.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, shuffle=True, batch_size=16, num_workers=1, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    for batch in data_loader:\n",
    "#         pass\n",
    "        assert len(batch) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __n\n",
    "    dataset = nc.SafeDataset(LMDBDataset('./data/lmdb-tmp.db'))\n",
    "    batch_size = 64\n",
    "    data_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=False)\n",
    "    n_epochs = 50\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for batch in data_loader:\n",
    "            assert len(batch) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
